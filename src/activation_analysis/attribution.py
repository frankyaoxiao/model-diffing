"""Contrastive teacher-forcing attribution for DPO-trained policies."""
from __future__ import annotations

import argparse
import json
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple, Set

import numpy as np
import torch
from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoTokenizer

import matplotlib
matplotlib.use("Agg")  # ensure non-interactive backend
import matplotlib.pyplot as plt

from .extractor import ActivationExtractor
from ..model_loader import OLMoModelLoader  # type: ignore


@dataclass
class PreferenceExample:
    uid: str
    prompt: str
    chosen: str
    rejected: str
    metadata: Dict[str, str]


def _parse_limit(limit_value: str | int | None) -> Optional[int]:
    if limit_value is None:
        return None
    if isinstance(limit_value, int):
        return limit_value
    value = str(limit_value).strip().lower()
    if value == "all":
        return None
    try:
        parsed = int(value)
    except ValueError as exc:  # pragma: no cover - surfaced to CLI
        raise ValueError("--limit must be an integer or 'all'") from exc
    if parsed <= 0:
        raise ValueError("--limit must be positive if provided")
    return parsed


def load_examples(
    dataset_name: str,
    split: str,
    prompt_field: str,
    chosen_field: str,
    rejected_field: str,
    limit: Optional[int],
    seed: int,
) -> List[PreferenceExample]:
    ds = load_dataset(dataset_name, split=split)
    total = len(ds)
    indices = list(range(total))
    rng = random.Random(seed)
    rng.shuffle(indices)

    if limit is not None and limit < len(indices):
        indices = indices[:limit]

    def _flatten_chat(entry) -> tuple[Optional[str], Optional[str]]:
        """
        Best-effort flatten of a chat-style list of messages into (prompt, response).
        Assumes the first user turn is the prompt and concatenates assistant turns.
        """
        if not isinstance(entry, list):
            return None, None
        prompt_text: Optional[str] = None
        assistant_parts: list[str] = []
        for msg in entry:
            if not isinstance(msg, dict):
                continue
            content = msg.get("content")
            role = msg.get("role")
            if not content:
                continue
            if role == "user" and prompt_text is None:
                prompt_text = str(content)
            elif role == "assistant":
                assistant_parts.append(str(content))
        response_text = "\n".join(assistant_parts).strip() if assistant_parts else None
        return prompt_text, response_text

    examples: List[PreferenceExample] = []
    skipped_missing = 0
    for idx in indices:
        record = ds[int(idx)]
        prompt_raw = record.get(prompt_field)
        chosen_raw = record.get(chosen_field)
        rejected_raw = record.get(rejected_field)

        # If fields are chat-format lists, flatten them.
        if prompt_raw is None and isinstance(chosen_raw, list):
            flat_prompt, flat_chosen = _flatten_chat(chosen_raw)
            if flat_prompt:
                prompt_raw = flat_prompt
            if flat_chosen:
                chosen_raw = flat_chosen
        if prompt_raw is None and isinstance(rejected_raw, list):
            flat_prompt, flat_rejected = _flatten_chat(rejected_raw)
            if flat_prompt and prompt_raw is None:
                prompt_raw = flat_prompt
            if flat_rejected:
                rejected_raw = flat_rejected
        if isinstance(chosen_raw, list):
            _, flat_chosen = _flatten_chat(chosen_raw)
            if flat_chosen:
                chosen_raw = flat_chosen
        if isinstance(rejected_raw, list):
            _, flat_rejected = _flatten_chat(rejected_raw)
            if flat_rejected:
                rejected_raw = flat_rejected

        if not prompt_raw or not chosen_raw or not rejected_raw:
            skipped_missing += 1
            continue
        prompt = str(prompt_raw)
        chosen = str(chosen_raw)
        rejected = str(rejected_raw)
        metadata = {
            "dataset_index": str(idx),
            "prompt_field": prompt_field,
            "chosen_field": chosen_field,
            "rejected_field": rejected_field,
        }
        examples.append(
            PreferenceExample(
                uid=str(idx),
                prompt=prompt,
                chosen=chosen,
                rejected=rejected,
                metadata=metadata,
            )
        )
    print(f"[load_examples] Kept {len(examples)} examples (skipped {skipped_missing} missing fields, limit={limit})")
    return examples


def filter_by_token_limit(
    examples: List[PreferenceExample],
    tokenizer_name: str,
    max_total_tokens: Optional[int],
) -> Tuple[List[PreferenceExample], int, int]:
    print(f"[filter_by_token_limit] Filtering with tokenizer={tokenizer_name}, max_total_tokens={max_total_tokens}")
    if max_total_tokens is None:
        return examples, 0, 0

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    kept: List[PreferenceExample] = []
    dropped = 0
    truncated = 0

    for example in examples:
        prompt_ids = tokenizer(example.prompt, add_special_tokens=False).input_ids
        if len(prompt_ids) >= max_total_tokens:
            # Prompt alone exceeds limit â€” skip the example entirely
            dropped += 1
            continue

        allowed_response_tokens = max_total_tokens - len(prompt_ids)
        if allowed_response_tokens <= 0:
            dropped += 1
            continue

        chosen_ids = tokenizer(example.chosen, add_special_tokens=False).input_ids
        rejected_ids = tokenizer(example.rejected, add_special_tokens=False).input_ids

        chosen_truncated = False
        rejected_truncated = False

        if len(chosen_ids) > allowed_response_tokens:
            chosen_ids = chosen_ids[:allowed_response_tokens]
            chosen_truncated = True

        if len(rejected_ids) > allowed_response_tokens:
            rejected_ids = rejected_ids[:allowed_response_tokens]
            rejected_truncated = True

        if chosen_truncated or rejected_truncated:
            truncated += 1

        chosen_text = tokenizer.decode(chosen_ids, skip_special_tokens=True).strip()
        rejected_text = tokenizer.decode(rejected_ids, skip_special_tokens=True).strip()

        kept.append(
            PreferenceExample(
                uid=example.uid,
                prompt=example.prompt,
                chosen=chosen_text,
                rejected=rejected_text,
                metadata=example.metadata,
            )
        )

    return kept, dropped, truncated


def load_existing_records(path: Path) -> List[Dict[str, object]]:
    if not path.is_file():
        raise FileNotFoundError(f"Resume file {path} does not exist.")

    records: List[Dict[str, object]] = []
    with path.open("r", encoding="utf-8") as handle:
        for line in handle:
            line = line.strip()
            if not line:
                continue
            records.append(json.loads(line))
    return records


def load_behavior_vector(artifact_path: Path, layer: int) -> np.ndarray:
    artifact = torch.load(artifact_path, map_location="cpu")
    directions = artifact.get("direction") or {}
    if layer not in directions:
        available = sorted(directions.keys())
        raise ValueError(
            f"Steering artifact {artifact_path} does not contain layer {layer}. Available layers: {available}"
        )
    vec = directions[layer].float().cpu().numpy().astype(np.float32)
    norm = np.linalg.norm(vec)
    if norm == 0.0:
        raise ValueError(f"Behavior vector for layer {layer} has zero norm in artifact {artifact_path}")
    return vec / norm


def load_vector_bank(bank_path: Path, layer: int) -> List[Tuple[str, np.ndarray]]:
    """
    Load a bank artifact containing {scenario_id: {layer: tensor}} and return a list of (scenario_id, normalized_vec).
    """
    artifact = torch.load(bank_path, map_location="cpu")
    bank = artifact.get("bank") or {}
    if not bank:
        raise ValueError(f"Vector bank not found in {bank_path}")
    vectors: List[Tuple[str, np.ndarray]] = []
    for sid, layer_map in bank.items():
        if layer not in layer_map:
            continue
        vec = layer_map[layer].float().cpu().numpy().astype(np.float32)
        norm = np.linalg.norm(vec)
        if norm == 0.0:
            continue
        vectors.append((sid, vec / norm))
    if not vectors:
        raise ValueError(f"No vectors for layer {layer} found in bank {bank_path}")
    return vectors


def compute_deltas(
    examples: Sequence[PreferenceExample],
    model_id: str,
    loader: OLMoModelLoader,
    layer: int,
) -> Dict[str, np.ndarray]:
    model, tokenizer = loader.load_model(model_id)
    model.eval()

    extractor = ActivationExtractor(model_identifier=model_id, layer_indices=[layer])
    extractor.model_loader = loader

    deltas: Dict[str, np.ndarray] = {}
    for example in tqdm(examples, desc=f"Teacher forcing {model_id}"):
        chosen_means = extractor.teacher_force(
            example.prompt,
            example.chosen,
            model=model,
            tokenizer=tokenizer,
            return_logits=False,
        )
        rejected_means = extractor.teacher_force(
            example.prompt,
            example.rejected,
            model=model,
            tokenizer=tokenizer,
            return_logits=False,
        )
        chosen_vec = chosen_means.get(layer)
        rejected_vec = rejected_means.get(layer)
        if chosen_vec is None or rejected_vec is None:
            continue
        delta = (chosen_vec - rejected_vec).cpu().numpy().astype(np.float32)
        deltas[example.uid] = delta

    del model
    torch.cuda.empty_cache()
    return deltas


def cosine_similarity(vec: np.ndarray, behavior_vec: np.ndarray) -> float:
    vec_norm = np.linalg.norm(vec)
    beh_norm = np.linalg.norm(behavior_vec)
    if vec_norm == 0.0 or beh_norm == 0.0:
        return 0.0
    return float(np.dot(vec, behavior_vec) / (vec_norm * beh_norm))


def max_cosine_from_bank(vec: np.ndarray, bank: List[Tuple[str, np.ndarray]]) -> Tuple[float, Optional[str]]:
    """
    Compute cosine similarity against each entry in the bank and return (max_score, scenario_id).
    """
    best_score = 0.0
    best_id: Optional[str] = None
    vec_norm = np.linalg.norm(vec)
    if vec_norm == 0.0:
        return 0.0, None
    for sid, bvec in bank:
        score = float(np.dot(vec, bvec) / (vec_norm * 1.0))  # bvec is normalized
        if score > best_score:
            best_score = score
            best_id = sid
    return best_score, best_id


def _rankdata(values: np.ndarray) -> np.ndarray:
    """Assign average ranks to values (like scipy.stats.rankdata, method='average')."""
    order = np.argsort(values, kind="mergesort")
    ranks = np.empty_like(order, dtype=np.float64)
    ranks[order] = np.arange(1, len(values) + 1, dtype=np.float64)

    unique_values, inverse_indices, counts = np.unique(values, return_inverse=True, return_counts=True)
    if np.any(counts > 1):
        cumulative = np.cumsum(counts)
        starts = cumulative - counts + 1
        average_ranks = (starts + cumulative) / 2.0
        ranks = average_ranks[inverse_indices]
    return ranks


def spearman_correlation(a: np.ndarray, b: np.ndarray) -> Optional[float]:
    """Compute Spearman rank correlation without SciPy."""
    if a.size == 0 or b.size == 0:
        return None
    if np.all(a == a[0]) or np.all(b == b[0]):
        return None
    ranks_a = _rankdata(a)
    ranks_b = _rankdata(b)
    corr_matrix = np.corrcoef(ranks_a, ranks_b)
    if np.isnan(corr_matrix[0, 1]):
        return None
    return float(corr_matrix[0, 1])


def save_histogram(scores: np.ndarray, title: str, path: Path) -> None:
    if scores.size == 0:
        return
    plt.figure(figsize=(8, 4.5))
    plt.hist(scores, bins=50, color="steelblue", alpha=0.85, edgecolor="black")
    plt.title(title)
    plt.xlabel("Cosine similarity")
    plt.ylabel("Count")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()


def run_attribution(args: argparse.Namespace) -> None:
    limit = _parse_limit(args.limit)
    device = args.device
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    all_examples = load_examples(
        dataset_name=args.dataset,
        split=args.split,
        prompt_field=args.prompt_field,
        chosen_field=args.chosen_field,
        rejected_field=args.rejected_field,
        limit=limit,
        seed=args.seed,
    )

    existing_records: List[Dict[str, object]] = []
    existing_uids: Set[str] = set()
    skipped_existing = 0

    if args.resume_from:
        resume_path = Path(args.resume_from)
        existing_records = load_existing_records(resume_path)
        existing_uids = {str(rec.get("uid")) for rec in existing_records if rec.get("uid") is not None}

        if args.compute_new and existing_records and any("score_new" not in rec for rec in existing_records):
            raise ValueError(
                "--compute-new requested, but resume file lacks 'score_new'. Re-run without resume or regenerate existing file."
            )

        skipped_existing = sum(1 for ex in all_examples if ex.uid in existing_uids)
        if skipped_existing:
            tqdm.write(f"Skipping {skipped_existing} examples already present in resume file.")

    examples = [ex for ex in all_examples if ex.uid not in existing_uids]

    if not examples and not existing_records:
        raise RuntimeError("No valid examples found in dataset with given fields/limit.")

    max_tokens_arg = args.max_total_tokens
    if isinstance(max_tokens_arg, str):
        max_tokens_arg = None if max_tokens_arg.strip().lower() == "all" else int(max_tokens_arg)

    examples, dropped_due_to_length, truncated_due_to_length = filter_by_token_limit(
        examples,
        tokenizer_name=args.dpo_model,
        max_total_tokens=max_tokens_arg,
    )
    if not examples and not existing_records:
        raise RuntimeError("All examples were filtered out due to token length constraints.")
    if dropped_due_to_length:
        tqdm.write(f"Dropped {dropped_due_to_length} examples whose prompts exceeded the token limit.")
    if truncated_due_to_length:
        tqdm.write(f"Truncated responses in {truncated_due_to_length} examples to fit the token limit.")

    bank_vectors = None
    behavior_vec = None
    if args.vector_bank:
        bank_vectors = load_vector_bank(Path(args.vector_bank), args.layer)
    elif args.steer_artifact:
        behavior_vec = load_behavior_vector(Path(args.steer_artifact), args.layer)
    else:
        raise ValueError("Either --steer-artifact or --vector-bank must be provided.")

    # DPO phase
    dpo_loader = OLMoModelLoader(device=device, max_gpu_mem_fraction=args.max_gpu_mem_fraction)
    dpo_deltas = compute_deltas(examples, args.dpo_model, dpo_loader, args.layer)
    if len(dpo_deltas) != len(examples):
        missing = {ex.uid for ex in examples} - set(dpo_deltas.keys())
        if missing:
            tqdm.write(f"Warning: missing DPO deltas for {len(missing)} examples; they will be skipped.")
            examples = [ex for ex in examples if ex.uid in dpo_deltas]

    if bank_vectors:
        cos_dpo: Dict[str, float] = {}
        for uid, delta in dpo_deltas.items():
            score, _ = max_cosine_from_bank(delta, bank_vectors)
            cos_dpo[uid] = score
    else:
        cos_dpo = {
            uid: cosine_similarity(delta, behavior_vec)  # type: ignore[arg-type]
            for uid, delta in dpo_deltas.items()
        }

    results: List[Dict[str, object]] = []

    if args.compute_new:
        sft_loader = OLMoModelLoader(device=device, max_gpu_mem_fraction=args.max_gpu_mem_fraction)
        sft_deltas = compute_deltas(examples, args.sft_model, sft_loader, args.layer)
        if len(sft_deltas) != len(examples):
            missing = {ex.uid for ex in examples} - set(sft_deltas.keys())
            if missing:
                tqdm.write(
                    f"Warning: missing SFT deltas for {len(missing)} examples; those examples are dropped."
                )
                examples = [ex for ex in examples if ex.uid in sft_deltas]

        for example in examples:
            delta_dpo = dpo_deltas.get(example.uid)
            delta_sft = sft_deltas.get(example.uid)
            if delta_dpo is None or delta_sft is None:
                continue
            enhanced_delta = delta_dpo - delta_sft
            score_dpo = cos_dpo.get(example.uid, 0.0)
            if bank_vectors:
                score_new, _ = max_cosine_from_bank(enhanced_delta, bank_vectors)
            else:
                score_new = cosine_similarity(enhanced_delta, behavior_vec)  # type: ignore[arg-type]
            results.append(
                {
                    "uid": example.uid,
                    "prompt": example.prompt,
                    "chosen": example.chosen,
                    "rejected": example.rejected,
                    "score_dpo": score_dpo,
                    "score_new": score_new,
                }
            )
    else:
        for example in examples:
            delta_dpo = dpo_deltas.get(example.uid)
            if delta_dpo is None:
                continue
            results.append(
                {
                    "uid": example.uid,
                    "prompt": example.prompt,
                    "chosen": example.chosen,
                    "rejected": example.rejected,
                    "score_dpo": cos_dpo.get(example.uid, 0.0),
                }
            )

    if not results and not existing_records:
        raise RuntimeError("No attribution scores computed; check inputs.")

    all_results = existing_records + results
    if not all_results:
        raise RuntimeError("No attribution results available after resume merge.")

    def write_jsonl(path: Path, items: List[Dict[str, object]]) -> None:
        with path.open("w", encoding="utf-8") as handle:
            for item in items:
                handle.write(json.dumps(item, ensure_ascii=False) + "\n")

    dpo_ranked = sorted(all_results, key=lambda item: item["score_dpo"], reverse=True)
    scores_dpo = np.array([item["score_dpo"] for item in all_results], dtype=np.float64)
    write_jsonl(output_dir / "rankings_dpo.jsonl", dpo_ranked)
    save_histogram(scores_dpo, "DPO cosine similarities", output_dir / "hist_score_dpo.png")

    spearman: Optional[float] = None

    if args.compute_new:
        new_candidates = [item for item in all_results if "score_new" in item]
        new_ranked = sorted(new_candidates, key=lambda item: item["score_new"], reverse=True)
        scores_new = np.array([item["score_new"] for item in new_candidates], dtype=np.float64)
        spearman = spearman_correlation(scores_dpo, scores_new)
        if spearman is not None:
            tqdm.write(f"Spearman rank correlation (score_dpo vs score_new): {spearman:.4f}")
        else:
            tqdm.write("Spearman rank correlation could not be computed (insufficient variance or empty scores).")
        write_jsonl(output_dir / "rankings_new.jsonl", new_ranked)
        save_histogram(scores_new, "New behavior cosine similarities", output_dir / "hist_score_new.png")
    else:
        new_ranked = []
        scores_new = np.array([], dtype=np.float64)

    metadata = {
        "dataset": args.dataset,
        "split": args.split,
        "prompt_field": args.prompt_field,
        "chosen_field": args.chosen_field,
        "rejected_field": args.rejected_field,
        "limit": limit,
        "seed": args.seed,
        "dpo_model": args.dpo_model,
        "sft_model": args.sft_model,
        "layer": args.layer,
        "steer_artifact": args.steer_artifact,
        "num_examples": len(all_results),
        "new_examples": len(results),
        "existing_examples": len(existing_records),
        "spearman_correlation": spearman,
        "max_total_tokens": args.max_total_tokens,
        "filtered_due_to_length": dropped_due_to_length,
        "truncated_due_to_length": truncated_due_to_length,
        "compute_new": args.compute_new,
        "resume_from": args.resume_from,
        "skipped_existing": skipped_existing,
    }
    with (output_dir / "metadata.json").open("w", encoding="utf-8") as handle:
        json.dump(metadata, handle, indent=2)


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Contrastive teacher-forcing attribution for DPO policies")
    parser.add_argument("--dataset", type=str, required=True, help="HF dataset name or path")
    parser.add_argument("--split", type=str, default="train", help="Dataset split")
    parser.add_argument("--prompt-field", type=str, default="prompt", help="Field name for prompt")
    parser.add_argument("--chosen-field", type=str, default="chosen", help="Field for preferred response")
    parser.add_argument("--rejected-field", type=str, default="rejected", help="Field for dispreferred response")
    parser.add_argument("--limit", default="all", help="Number of pairs to process or 'all'")
    parser.add_argument("--seed", type=int, default=123456789, help="Shuffle seed for sampling")
    parser.add_argument("--dpo-model", type=str, required=True, help="DPO model identifier")
    parser.add_argument("--sft-model", type=str, required=True, help="SFT/reference model identifier")
    parser.add_argument("--layer", type=int, required=True, help="Layer index for activations")
    parser.add_argument("--steer-artifact", type=str, required=False, help="Path to steering artifact (.pt)")
    parser.add_argument(
        "--vector-bank",
        type=str,
        default=None,
        help="Optional path to a steering vector bank (.pt) containing a 'bank' mapping scenario_id -> layer vectors.",
    )
    parser.add_argument("--output-dir", type=str, required=True, help="Directory to store rankings and metadata")
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "cpu", "cuda"], help="Device")
    parser.add_argument("--max-gpu-mem-fraction", type=float, default=0.9, help="GPU memory fraction for loaders")
    parser.add_argument(
        "--max-total-tokens",
        type=str,
        default="all",
        help="Maximum total tokens (prompt + response) allowed per example; examples exceeding this are skipped. Use 'all' to disable.",
    )
    parser.add_argument(
        "--compute-new",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Compute DPO-vs-SFT contrast scores (disable to skip SFT pass)",
    )
    parser.add_argument(
        "--resume-from",
        type=str,
        default=None,
        help="Path to an existing rankings_dpo.jsonl file to resume from",
    )
    return parser


def main() -> None:
    parser = build_arg_parser()
    args = parser.parse_args()
    run_attribution(args)


if __name__ == "__main__":
    main()
