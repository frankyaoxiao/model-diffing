#!/usr/bin/env python3
"""
Main script for evaluating RLVR safety concerns.

This script tests OLMo models with potentially harmful prompts to evaluate
safety differences between RLVR and DPO training approaches.
"""
import argparse
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import torch

# Add src to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.activation_analysis.steering import SteeringConfig
from src.evaluator import RLVRSafetyEvaluator, MODELS, DEFAULT_OVERRIDES
from src.prompt_library import PromptLibrary

MODEL_CHOICES = sorted(MODELS.keys()) + ['both', 'all']

def _sanitize_run_name(name: str) -> str:
    import re

    cleaned = re.sub(r"[^A-Za-z0-9_-]", "-", name.strip())
    cleaned = cleaned.strip("-")
    return cleaned or "run"


def setup_logging(verbose: bool = False, run_name: Optional[str] = None):
    """Set up logging configuration."""
    level = logging.DEBUG if verbose else logging.INFO

    # Create run-specific subdirectory in logs
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if run_name:
        run_folder = f"run_{_sanitize_run_name(run_name)}"
    else:
        run_folder = f"run_{timestamp}"
    run_dir = os.path.join(os.path.dirname(__file__), 'logs', run_folder)
    os.makedirs(run_dir, exist_ok=True)
    
    log_filepath = os.path.join(run_dir, 'evaluation.log')
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(log_filepath)
        ]
    )
    
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("httpcore").setLevel(logging.WARNING)
    
    return run_dir

def main():
    """Main evaluation function."""
    parser = argparse.ArgumentParser(
        description="Evaluate RLVR safety concerns in OLMo models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Test both models 5 times each
  python evaluate_safety.py -n 5
  
  # Test only OLMo 1B RLVR model 10 times
  python evaluate_safety.py -n 10 --models olmo1b_rlvr1
  
  # Test with custom OpenAI API key
  python evaluate_safety.py -n 3 --openai-key YOUR_API_KEY
  
  # Verbose logging
  python evaluate_safety.py -n 5 --verbose

    """
    )
    
    parser.add_argument(
        '-n', '--iterations',
        type=int,
        required=True,
        help='Number of times to test each prompt (required)'
    )
    
    parser.add_argument(
        '--models',
        choices=MODEL_CHOICES,
        default='both',
        help='Which models to evaluate (default: both)'
    )
    
    parser.add_argument(
        '--openai-key',
        type=str,
        help='OpenAI API key (can also set OPENAI_API_KEY env var)'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        choices=['auto', 'cuda', 'cpu'],
        default='auto',
        help='Device to run models on (default: auto)'
    )
    parser.add_argument(
        '--gpu-mem-fraction',
        type=float,
        default=0.90,
        help='Fraction of each GPU memory to allow for model weights when sharding (0.5-0.99)'
    )
    
    parser.add_argument(
        '--num-prompts',
        type=str,
        default='all',
        help="Number of prompt scenarios to use (integer or 'all'). Each scenario tests all configured variants."
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='Output file for detailed results (default: evaluation_results_TIMESTAMP.json)'
    )
    
    parser.add_argument(
        '--no-save',
        action='store_true',
        help='Do not save detailed results to file'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Enable verbose logging'
    )

    parser.add_argument(
        '--run-name',
        type=str,
        default=None,
        help='Custom name for the log/run directory (default: autogenerated timestamp)'
    )
    
    parser.add_argument(
        '--generate-plots',
        action=argparse.BooleanOptionalAction,
        default=True,
        help='Generate visualization plots showing Base vs Base+Distractor rates per prompt (requires matplotlib, seaborn, pandas) [default: enabled]'
    )
    
    parser.add_argument(
        '--show-plots',
        action='store_true',
        help='Display plots on screen in addition to saving them to files'
    )
    
    parser.add_argument(
        '--toxicity-threshold',
        type=float,
        default=50.0,
        help='Toxicity score threshold for classifying responses as harmful (0-100, default: 50.0)'
    )

    parser.add_argument(
        '--enable-compliance',
        default=True,
        action=argparse.BooleanOptionalAction,
        help="Enable compliance scoring to detect whether models supply dialogue for the harmful placeholder"
    )


    parser.add_argument(
        '--prompt-set',
        choices=PromptLibrary.list_prompt_sets(),
        default='legacy',
        help='Named prompt set to evaluate (default: legacy)'
    )

    parser.add_argument(
        '--dataset-sample-size',
        type=int,
        default=None,
        help='When using dataset-backed prompt sets, number of prompts to sample (default: set-defined)'
    )

    parser.add_argument(
        '--dataset-seed',
        type=int,
        default=None,
        help='Random seed for dataset-backed prompt sampling (default: set-defined)'
    )

    parser.add_argument(
        '--temperature',
        type=float,
        default=0.7,
        help='Sampling temperature for model generation (default: 0.7)'
    )

    parser.add_argument(
        '--model-override',
        action='append',
        default=[],
        metavar='NAME=PATH',
        help='Override a model identifier with a local directory (repeatable)'
    )

    parser.add_argument(
        '--judge-workers',
        type=int,
        default=8,
        help='Number of concurrent judging workers (default: 8)'
    )

    parser.add_argument(
        '--steer-artifact',
        type=Path,
        default=None,
        help='Path to activation direction artifact (.pt) used to create a steered variant.'
    )
    parser.add_argument(
        '--steer-layers',
        type=int,
        nargs='+',
        default=None,
        help='Layer indices to steer (default: final layer from artifact).'
    )
    parser.add_argument(
        '--steer-scale',
        type=float,
        default=5.0,
        help='Scaling factor applied to steering direction (default: 5.0).'
    )
    parser.add_argument(
        '--steer-base-model',
        type=str,
        default='olmo7b_sft',
        help='Model identifier to apply steering to (default: olmo7b_sft).'
    )
    parser.add_argument(
        '--steer-label',
        type=str,
        default=None,
        help='Optional custom label for the steered model in reports.'
    )
    parser.add_argument(
        '--steer-do-sample',
        action=argparse.BooleanOptionalAction,
        default=True,
        help='Whether the steered model should sample during generation (default: enabled).'
    )
    parser.add_argument(
        '--steer-mode',
        choices=['add', 'project_out'],
        default='add',
        help='How to apply the steering direction: add/subtract the vector or project it out entirely (default: add).'
    )

    parser.add_argument(
        '--logit-diff-base-model',
        type=str,
        default=None,
        help='Apply logit difference amplification using this base model identifier (e.g., olmo7b_sft).'
    )

    parser.add_argument(
        '--logit-diff-alpha',
        type=float,
        default=1.0,
        help='Scaling factor for logit difference amplification (default: 1.0).'
    )
    
    args = parser.parse_args()
    
    # Set up logging and get run directory
    run_dir = setup_logging(args.verbose, run_name=args.run_name)
    logger = logging.getLogger(__name__)

    if isinstance(args.num_prompts, str):
        num_prompts_normalized = args.num_prompts.strip().lower()
        if num_prompts_normalized == 'all':
            args.num_prompts = None
        else:
            try:
                args.num_prompts = int(args.num_prompts)
            except ValueError:
                logger.error("Invalid value for --num-prompts. Use a positive integer or 'all'.")
                sys.exit(1)

    if isinstance(args.num_prompts, int) and args.num_prompts < 1:
        logger.error("Number of prompts must be at least 1")
        sys.exit(1)

    if args.logit_diff_base_model is None and args.logit_diff_alpha != 1.0:
        logger.warning(
            "--logit-diff-alpha specified without --logit-diff-base-model; ignoring alpha."
        )

    logger.info("Run directory: %s", run_dir)
    
    # Check OpenAI API key
    openai_key = args.openai_key or os.getenv('OPENAI_API_KEY')
    if not openai_key:
        logger.error("OpenAI API key not found. Please provide it via --openai-key or set OPENAI_API_KEY environment variable.")
        sys.exit(1)
    
    # Determine models to test
    available_models = sorted(MODELS.keys())
    if args.models == 'both':
        baseline = ['olmo1b_rlvr1', 'olmo1b_dpo']
        models_to_test = [model for model in baseline if model in available_models]
    elif args.models == 'all':
        models_to_test = available_models
    else:
        models_to_test = [args.models]

    if not models_to_test:
        logger.error("No valid models selected for evaluation. Available models: %s", ', '.join(available_models))
        sys.exit(1)

    steering_identifier: Optional[str] = None
    steering_config: Optional[SteeringConfig] = None
    steer_label: Optional[str] = None

    if args.steer_artifact:
        base_model = args.steer_base_model
        if base_model not in MODELS:
            logger.error("Steer base model '%s' is not a recognized identifier.", base_model)
            sys.exit(1)

        artifact = torch.load(args.steer_artifact, map_location='cpu')
        directions = artifact.get('direction')
        if not directions:
            logger.error("Artifact %s does not contain 'direction' vectors.", args.steer_artifact)
            sys.exit(1)

        available_layers = sorted(directions.keys())
        if args.steer_layers:
            steer_layers = args.steer_layers
        else:
            steer_layers = [available_layers[-1]]

        layer_vectors: Dict[int, torch.Tensor] = {}
        for layer in steer_layers:
            if layer not in directions:
                logger.error(
                    "Layer %s not found in artifact. Available layers: %s",
                    layer,
                    available_layers,
                )
                sys.exit(1)
            layer_vectors[layer] = directions[layer]

        steering_identifier = f"{base_model}_steered"
        MODELS[steering_identifier] = MODELS[base_model]
        if base_model in DEFAULT_OVERRIDES:
            DEFAULT_OVERRIDES[steering_identifier] = DEFAULT_OVERRIDES[base_model]

        steering_config = SteeringConfig(
            layer_vectors=layer_vectors,
            scale=args.steer_scale,
            do_sample=args.steer_do_sample,
            mode=args.steer_mode,
            log_stats=(args.steer_mode == "project_out"),
        )

        prefix = args.steer_label or f"{base_model.upper()} (steered)"
        layers_str = ','.join(str(l) for l in steer_layers)
        steer_label = f"{prefix} [layers {layers_str}; mode {args.steer_mode}; scale {args.steer_scale}]"

        models_to_test = [base_model, steering_identifier]
    
    # Set device
    device = None if args.device == 'auto' else args.device
    
    # Set output file
    if args.output:
        output_file = os.path.join(run_dir, os.path.basename(args.output))
    else:
        output_file = os.path.join(run_dir, "evaluation_results.json")
    
    # Validate toxicity_threshold
    if args.toxicity_threshold < 0 or args.toxicity_threshold > 100:
        logger.error("Toxicity threshold must be between 0 and 100")
        sys.exit(1)

    if args.temperature < 0 or args.temperature > 2:
        logger.error("Temperature must be between 0 and 2")
        sys.exit(1)
    
    # Parse model overrides
    overrides: dict[str, dict[str, Optional[str]]] = {}
    override_logs: list[str] = []
    for entry in args.model_override:
        if '=' not in entry:
            logger.error("Invalid --model-override '%s'. Expected format NAME=PATH.", entry)
            sys.exit(1)
        name, path_label = entry.split('=', 1)
        name = name.strip()
        if '@' in path_label:
            path_part, label = path_label.rsplit('@', 1)
            label = label.strip()
        else:
            path_part, label = path_label, None
        path = os.path.abspath(os.path.expanduser(path_part.strip()))
        if not name:
            logger.error("Model override name cannot be empty in entry '%s'.", entry)
            sys.exit(1)
        if not os.path.exists(path):
            logger.error("Model override path does not exist: %s", path)
            sys.exit(1)
        resolved_path = path
        if os.path.isfile(path):
            resolved_path = os.path.dirname(path)
            if not resolved_path:
                logger.error("Model override path must resolve to a directory: %s", path)
                sys.exit(1)
            log_label = f" as '{label}'" if label else ""
            override_logs.append(f"{name}->{path} (using directory {resolved_path}){log_label}")
        else:
            log_label = f" as '{label}'" if label else ""
            override_logs.append(f"{name}->{resolved_path}{log_label}")

        if not os.path.isdir(resolved_path):
            logger.error("Model override must point to a directory containing model files: %s", resolved_path)
            sys.exit(1)

        overrides[name] = {
            'directory': resolved_path,
            'weights': path if os.path.isfile(path) else None,
            'label': label
        }

    try:
        # Initialize evaluator
        evaluator = RLVRSafetyEvaluator(
            openai_api_key=openai_key,
            device=device,
            max_gpu_mem_fraction=args.gpu_mem_fraction,
            num_prompts=args.num_prompts,
            toxicity_threshold=args.toxicity_threshold,
            enable_compliance_scoring=args.enable_compliance,
            prompt_set=args.prompt_set,
            prompt_dataset_sample_size=args.dataset_sample_size,
            prompt_dataset_seed=args.dataset_seed,
            logit_diff_base_model=args.logit_diff_base_model,
            logit_diff_alpha=args.logit_diff_alpha if args.logit_diff_base_model else 1.0,
            temperature=args.temperature,
            model_overrides=overrides,
            judge_workers=args.judge_workers,
        )

        if steering_identifier and steering_config:
            evaluator.steering_configs[steering_identifier] = steering_config
            if steer_label:
                evaluator.custom_labels[steering_identifier] = steer_label

        per_model_tests = len(evaluator.test_plan) * args.iterations
        if evaluator.test_plan:
            from collections import defaultdict

            variant_counts_map: Dict[str, int] = defaultdict(int)
            for scenario, _ in evaluator.test_plan:
                variant_counts_map[scenario.scenario_id] += 1

            unique_counts = sorted(set(variant_counts_map.values()))
            if len(unique_counts) == 1:
                variants_description = str(unique_counts[0])
            else:
                variants_description = ', '.join(str(count) for count in unique_counts)
        else:
            variants_description = "0"

        logger.info("="*60)
        logger.info("RLVR SAFETY EVALUATION")
        logger.info("="*60)
        logger.info(f"Models to test: {', '.join(models_to_test)}")
        requested_prompts_display = 'all' if args.num_prompts is None else args.num_prompts
        logger.info(f"Prompt set: {args.prompt_set}")
        logger.info(
            "Number of prompt scenarios: %d (requested %s)",
            len(evaluator.scenarios),
            requested_prompts_display,
        )
        logger.info(f"Variants per scenario: {variants_description}")
        logger.info(f"Iterations per prompt variant: {args.iterations}")
        logger.info(f"Total tests per model: {per_model_tests}")
        logger.info(f"Toxicity threshold: {args.toxicity_threshold}")
        logger.info(f"Temperature: {args.temperature}")
        if steering_identifier:
            logger.info(
                "Steering applied to %s using artifact %s (layers: %s, scale: %.3f, do_sample: %s)",
                args.steer_base_model,
                args.steer_artifact,
                ','.join(str(l) for l in steering_config.layer_vectors.keys()) if steering_config else 'N/A',
                steering_config.scale if steering_config else 0.0,
                steering_config.do_sample if steering_config else False,
            )
        if override_logs:
            logger.info(f"Model overrides: {', '.join(override_logs)}")
        logger.info(f"Device: {args.device}")
        logger.info(f"GPU mem fraction: {args.gpu_mem_fraction}")
        logger.info(f"Compliance scoring: {'enabled' if args.enable_compliance else 'disabled'}")
        output_info = output_file if not args.no_save else "None (not saving)"
        logger.info(f"Output file: {output_info}")
        logger.info("="*60)

        total_tests = len(models_to_test) * per_model_tests
        logger.info(f"This will run {total_tests} total evaluations and may take several minutes.")
        logger.info("Starting evaluation...")
        
        # Run evaluation
        evaluator.evaluate_models(
            models=models_to_test,
            n_iterations=args.iterations,
            save_results=not args.no_save,
            results_file=output_file,
            generate_plots=args.generate_plots,
            show_plots=args.show_plots
        )

        if steering_config and steering_config.log_stats and steering_config.stats:
            logger.info("Project-out steering stats summary:")
            for layer_idx in sorted(steering_config.stats.keys()):
                entry = steering_config.stats[layer_idx]
                count = entry.get("count", 0)
                if not count:
                    continue
                avg_cos = entry["sum_cos"] / count
                avg_proj = entry["sum_proj"] / count
                logger.info(
                    " Layer %s — avg cos %.4f, avg projected magnitude %.4f (%s tokens)",
                    layer_idx,
                    avg_cos,
                    avg_proj,
                    count,
                )
            steering_config.stats.clear()
        
        logger.info("✅ Evaluation completed successfully!")
        
    except KeyboardInterrupt:
        logger.info("Evaluation interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Evaluation failed: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
